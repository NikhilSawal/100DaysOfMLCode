---
title: "Multiple Linear Regression"
author: "Nikhil Sawal"
date: "August 21, 2018"
output: rmarkdown::github_document
---

## Loading libraries
```{r, warning=FALSE, message=FALSE}
library(MASS)
library(ISLR)
library(caTools)
library(corrplot)
library(car)

df <- Boston
attach(df)

```

## Test-train split - using `caTools` package
```{r}
set.seed(102)
split <- sample.split(df$medv, SplitRatio = 0.7)
train <- subset(df, split == T)
test <- subset(df, split == F)

```

## Model 1 [With all predictors]
Our first model of choice should be to include all variables and see how the model performs. Following lines of code will help us fit a model and dislplay the summary using the `summary()` function. The summary include estimates for our model coefficients i.e. all the `beta-hat's`, the standard error and the p-value based on t-statistic. It also outputs, values of the training R^2 and (adjusted R^2). We haven't talked about adjusted R^2 yet, but it's another and more reliable measure for checking model accuracy. The reason it is more reliable is that, unlike R^2, (adjusted R^2) penalizes if noise variables are added to the model, whereas R^2 keeps increasing as we add more variables to our model.
```{r}
lm.fit <- lm(medv~., data = train)
summary(lm.fit)
```
Looking at the p-values, it seems like variables `crim`, `indus` and `age` are not useful in predicting the response. So, we try to fit our second model excluding these variables.  

## Model 2 [With only significant predictors]
```{r}
lm.fit1 <- lm(medv~.-crim-indus-age, data = train)
summary(lm.fit1)
```
There are negligible differences between `R^2` and `(adjusted R^2)` of the two models. These are values for the training set and we really care about the performance of the model on the test set. In the following section, we develop a function that evaluated the model performance. 

## Model Evaluation Function, `model_eval`

This function is written for reusability. Notice that every time we fit a new model, we would need to repeat the following operations over and over again.  

* Make predictions & store them in a data frame with the observed values
* Replace negatives by zeros, if any in the predictions
* Coompute MSE and R^2

Wrapping these operations in a function, will save us a lot of time, since all we need to do is call the function, `model_eval` and pass in the new model we fit and it will return the test MSE and R^2 in the form of a list. 


```{r}
model_eval <- function(model){
  
  # Prediction
  predictions <- predict(model, test)
  results <- cbind(predictions, test$medv)
  colnames(results) <- c('Predictions', 'Observed')
  results <- as.data.frame(results)
  
  # Function to replace negatives with 0
  make_zero <- function(x){
    if(x<0){
      return(0)
    }else{
      return(x)
    }
  }
  
  results$Predictions <- sapply(results$Predictions, make_zero)
  
  # Mean squared error
  mse <- mean((results$Predictions - results$Observed)^2)

  # R^2
  SSRes <- sum((results$Predictions-results$Observed)^2)
  SST <- sum((results$Observed-mean(results$Observed))^2)
  
  r2 <- 1 - (SSRes/SST) 
  return(list(mse, r2))
  
}

```

## Interaction terms
The previous two models made a serious assumption, that the relationship between response `Y` and predictor `X` is additive, i.e. the effect of changes in a predictor variable `Xj` on the response `Y`, is independent of the values of other predictors. We relax this assumption by including the interaction terms.  

**How do we choose the interaction terms?**  

The best way to choose interaction terms is to use domain knowledge. For the sake of simplicity, we check the correlation between the predictor variables and select the variables that have a correlation > 0.7
We use the `corrplot()` library in R to visualize the correlations. Once we identify a set of variables that are highly correlated, we include their interaction terms in our model and refit. The following lines of code and plot will help us visualize.
```{r}
M <- cor(df)
corrplot(M, method = "circle")
```

From the above plot, we observe strong intractions like `indus*nox, indus*dis, indus*tax, nox*age, noc*dis, ade*dis, rad*tax`. We refit the model using the main effects and interaction and check the summary. 

```{r}
lm.fit2 <- lm(medv ~ crim + zn + chas + rm + ptratio + black + lstat +
                indus*nox + indus*dis + indus*tax + nox*age + nox*dis + 
                age*dis + rad*tax)
summary(lm.fit2)
```
There is very little difference between the R^2 value of this model and the previous model. Let's see how each of the model performs on test set, using the `model_eval` function, that we developed in previous section. 

## MSE and R2
### Model 0
```{r}
model_eval(lm.fit)
```
### Model 1
```{r}
model_eval(lm.fit1)
```
### Model 2
```{r}
model_eval(lm.fit2)
```
Model 2 has shown good improvement in the test MSE and R^2 value. The MSE has reduced a little but the test R^2 has shown ~8% improvement. 

# Model adequacy checks

We specifically check the adequacy of model 2, since it comes more closer to a real world situation.
To check the adequacy of the model, we will plot residuals vs. fitted values, to see if our assumptions about, linear relationship betweeen Y & X and constant variance of error terms, hold. We will also plot the normal probability plot to check the normality assumption. 
```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

* The plot in the top left panel is the plot of residuals vs. fitted values. It shows a non-linear relationship, implying that our assumption of the true model being linear is flawed. We can correct this by performing some transformations. 

* The figure in the top right corner suggests that our normality assumption is also violated, since the residuals don't fall along the straight line.

# Multicollinearity
As we saw on day 14, we can use VIF to check if multicollinearity exists. Multicollinearity refers to a near-linear dependency between predictor variables and response. If there is multicollinearity in our model, **then there would be a large amount of variance in our predictor varibles and hence our predictions would change drastically for different data sets**. So, it's really important to check for multicollinearity. As noted in the infographic, multicollinearity exists if VIF > 5 or 10. We use the `vif()`
function from the `car` library on our model.
```{r}
vif(lm.fit2)
```
From the above output, we can see that we have a lot of predictors with values >> 5 & 10. This implies serious multicollinearity and we need some measuere to treat it. 
