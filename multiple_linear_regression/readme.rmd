---
title: "Multiple Linear Regression"
author: "Nikhil Sawal"
date: "August 21, 2018"
output: rmarkdown::github_document
---

## Loading libraries
```{r}
library(MASS)
library(ISLR)
library(caTools)
library(corrplot)

df <- Boston
attach(df)

```

## Test-train split
```{r}
set.seed(102)
split <- sample.split(df$medv, SplitRatio = 0.7)
train <- subset(df, split == T)
test <- subset(df, split == F)

```

## Model 1 [With all predictors]
```{r}
lm.fit <- lm(medv~., data = train)
summary(lm.fit)
```

## Model 2 [With only significant predictors]
```{r}
lm.fit1 <- lm(medv~.-crim-indus-age, data = train)
summary(lm.fit1)
```

## Model Evaluation Function, `model_eval`

This function is written for reusability. Notice that every time we fit a new model, we would need to repeat the following operations over and over again.  

* Make predictions & store them in a data frame with the observed values
* Replace negatives by zeros, if any in the predictions
* Coompute MSE and R^2

Wrapping these operations in a function, will save us a lot of time, since all we need to do is call the function, `model_eval` and pass in the new model we fit and it will return the test MSE and R^2 in the form of a list. 


```{r}
model_eval <- function(model){
  
  # Prediction
  predictions <- predict(model, test)
  results <- cbind(predictions, test$medv)
  colnames(results) <- c('Predictions', 'Observed')
  results <- as.data.frame(results)
  
  # Function to replace negatives with 0
  make_zero <- function(x){
    if(x<0){
      return(0)
    }else{
      return(x)
    }
  }
  
  results$Predictions <- sapply(results$Predictions, make_zero)
  
  # Mean squared error
  mse <- mean((results$Predictions - results$Observed)^2)

  # R^2
  SSRes <- sum((results$Predictions-results$Observed)^2)
  SST <- sum((results$Observed-mean(results$Observed))^2)
  
  r2 <- 1 - (SSRes/SST) 
  return(list(mse, r2))
  
}

```

## Interaction terms
The previous two models made a serious assumption, that the relationship between response `Y` and predictor `X` is additive, i.e. the effect of changes in a predictor variable `Xj` on the response `Y`, is independent of the values of other predictors. We relax this assumption by including the interaction terms.  

**How do we choose the interaction terms?**  

We check the correlation between the predictor variables and select the variables that have a correlation > 0.7
We use the `corrplot()` library in R to visualize the correlations. Once we identify a set of variables that are highly correlated, we include their interaction terms in our model and refit. The following lines of code and plot will help us visualize.
```{r}
M <- cor(df)
corrplot(M, method = "circle")
```

From the above plot, we observe strong intractions like `indus*nox, indus*dis, indus*tax, nox*age, noc*dis, ade*dis, rad*tax`. We refit the model using the main effects and interaction and check the summary. 

```{r}
lm.fit2 <- lm(medv ~ crim + zn + chas + rm + ptratio + black + lstat +
                indus*nox + indus*dis + indus*tax + nox*age + nox*dis + 
                age*dis + rad*tax)
summary(lm.fit2)
```
There is very little difference between the the R^2 value of this model and the previous model. Let
## MSE and R2
### Model 0
```{r}
model_eval(lm.fit)
```
### Model 1
```{r}
model_eval(lm.fit1)
```
### Model 2
```{r}
model_eval(lm.fit2)
```
Model 2 has shown good improvement in the test MSE and R^2 value. The MSE has reduced a little but the test R^2 has shown ~8% improvement. 

# Model adequacy checks
## Model 2

To check the adequacy of the model, we will plot residuals vs. fitted values, to see if our assumptions about, linear relationship betweeen Y & X and constant variance of error terms hold. 
```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

